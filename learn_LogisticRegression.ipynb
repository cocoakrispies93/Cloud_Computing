{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMoFgQWbkQWz"
      },
      "source": [
        "#Assignment 6 - Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "zGTz-1xKkQW1"
      },
      "source": [
        "## Binary Customer Churn\n",
        "\n",
        "A company has a lot of  customers that use their service to produce ads for the customer websites. They've noticed that they have quite a bit of churn in clients. They basically randomly assign account managers right now, but want you to create a machine learning model that will help predict which customers will churn (stop buying their service) so that they can correctly assign the customers most at risk to churn an account manager. Luckily they have some historical data, can you help them out? Create a classification algorithm that will help classify whether or not a customer churned. \n",
        "\n",
        "The data is saved as historical_data.csv. Here are the fields and their definitions:\n",
        "\n",
        "    Name : Name of the latest contact at Company\n",
        "    Age: Customer Age\n",
        "    Total_Purchase: Total Ads Purchased\n",
        "    Account_Manager: Binary 0=No manager, 1= Account manager assigned\n",
        "    Years: Totaly Years as a customer\n",
        "    Num_sites: Number of websites that use the service.\n",
        "    Onboard_date: Date that the name of the latest contact was onboarded\n",
        "    Location: Client HQ Address\n",
        "    Company: Name of Client Company\n",
        "    \n",
        "**NB:Create the model and evaluated it.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1: Install Dependencies\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.3.0-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "#Step 2: Add environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.3.0-bin-hadoop3\"\n",
        "\n",
        "#Step 3: Initialize Pyspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "#creating spark context\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "sc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "QIqYdS68cPtD",
        "outputId": "b79bf35f-44a9-4f30-dc23-f0761fe1ed21"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=pyspark-shell>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://190b59109657:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('historical').getOrCreate()\n",
        "\n",
        "#importing the logistic regression library\n",
        "from pyspark.ml.classification import LogisticRegression"
      ],
      "metadata": {
        "id": "XoB-PRcEcZQk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training data\n",
        "training = spark.read.csv('historical_data.csv',inferSchema=True,header=True)\n",
        "\n",
        "#lr = LogisticRegression()\n",
        "\n",
        "# Fit the model\n",
        "#lrModel = lr.fit(training)\n",
        "\n",
        "#showing the schema\n",
        "training.printSchema()\n",
        "training.show()\n",
        "\n",
        "#trainingSummary = lrModel.summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQZs5d2ncmne",
        "outputId": "4ce6a84b-4348-419d-d1b5-69d48d203797"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Names: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- Total_Purchase: double (nullable = true)\n",
            " |-- Account_Manager: integer (nullable = true)\n",
            " |-- Years: double (nullable = true)\n",
            " |-- Num_Sites: integer (nullable = true)\n",
            " |-- Onboard_date: string (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            " |-- Company: string (nullable = true)\n",
            " |-- Churn: integer (nullable = true)\n",
            "\n",
            "+-------------------+---+--------------+---------------+-----+---------+----------------+--------------------+--------------------+-----+\n",
            "|              Names|Age|Total_Purchase|Account_Manager|Years|Num_Sites|    Onboard_date|            Location|             Company|Churn|\n",
            "+-------------------+---+--------------+---------------+-----+---------+----------------+--------------------+--------------------+-----+\n",
            "|   Cameron Williams| 42|       11066.8|              0| 7.22|        8|  8/30/2013 7:00|10265 Elizabeth M...|          Harvey LLC|    1|\n",
            "|      Kevin Mueller| 41|      11916.22|              0|  6.5|       11|  8/13/2013 0:38|6157 Frank Garden...|          Wilson PLC|    1|\n",
            "|        Eric Lozano| 38|      12884.75|              0| 6.67|       12|  6/29/2016 6:20|1331 Keith Court ...|Miller, Johnson a...|    1|\n",
            "|      Phillip White| 42|       8010.76|              0| 6.71|       10| 4/22/2014 12:43|13120 Daniel Moun...|           Smith Inc|    1|\n",
            "|     Cynthia Norton| 37|       9191.58|              0| 5.56|        9| 1/19/2016 15:31|765 Tricia Row Ka...|          Love-Jones|    1|\n",
            "|   Jessica Williams| 48|      10356.02|              0| 5.12|        8|  3/3/2009 23:13|6187 Olson Mounta...|        Kelly-Warren|    1|\n",
            "|        Eric Butler| 44|      11331.58|              1| 5.23|       11|  12/5/2016 3:35|4846 Savannah Roa...|   Reynolds-Sheppard|    1|\n",
            "|      Zachary Walsh| 32|       9885.12|              1| 6.92|        9|  3/9/2006 14:50|25271 Roy Express...|          Singh-Cole|    1|\n",
            "|        Ashlee Carr| 43|       14062.6|              1| 5.46|       11|  9/29/2011 5:47|3725 Caroline Str...|           Lopez PLC|    1|\n",
            "|     Jennifer Lynch| 40|       8066.94|              1| 7.11|       11| 3/28/2006 15:42|363 Sandra Lodge ...|       Reed-Martinez|    1|\n",
            "|       Paula Harris| 30|      11575.37|              1| 5.22|        8|11/13/2016 13:13|Unit 8120 Box 916...|Briggs, Lamb and ...|    1|\n",
            "|     Bruce Phillips| 45|       8771.02|              1| 6.64|       11| 5/28/2015 12:14|Unit 1895 Box 094...|    Figueroa-Maynard|    1|\n",
            "|       Craig Garner| 45|       8988.67|              1| 4.84|       11|  2/16/2011 8:10|897 Kelley Overpa...|     Abbott-Thompson|    1|\n",
            "|       Nicole Olson| 40|       8283.32|              1|  5.1|       13| 11/22/2012 5:35|11488 Weaver Cape...|Smith, Kim and Ma...|    1|\n",
            "|     Harold Griffin| 41|       6569.87|              1|  4.3|       11|  3/28/2015 2:13|1774 Peter Row Ap...|Snyder, Lee and M...|    1|\n",
            "|       James Wright| 38|      10494.82|              1| 6.81|       12|  7/22/2015 8:38|45408 David Path ...|      Sanders-Pierce|    1|\n",
            "|      Doris Wilkins| 45|       8213.41|              1| 7.35|       11|   9/3/2006 6:13|28216 Wright Moun...|Andrews, Adams an...|    1|\n",
            "|Katherine Carpenter| 43|      11226.88|              0| 8.08|       12| 10/22/2006 4:42|Unit 4948 Box 481...|Morgan, Phillips ...|    1|\n",
            "|     Lindsay Martin| 53|       5515.09|              0| 6.85|        8|  10/7/2015 0:27|69203 Crosby Divi...|      Villanueva LLC|    1|\n",
            "|        Kathy Curry| 46|        8046.4|              1| 5.69|        8| 11/6/2014 23:47|9569 Caldwell Cre...|Berry, Orr and Ca...|    1|\n",
            "+-------------------+---+--------------+---------------+-----+---------+----------------+--------------------+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training.select('Company').show(30)\n",
        "#checking if there are companies that can be grouped together and discretized, but they're unique\n",
        "#same problem with names, dates, and addresses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hRy1lNGl66B",
        "outputId": "8c25a610-f091-4037-f0e4-1d902340a54a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|             Company|\n",
            "+--------------------+\n",
            "|          Harvey LLC|\n",
            "|          Wilson PLC|\n",
            "|Miller, Johnson a...|\n",
            "|           Smith Inc|\n",
            "|          Love-Jones|\n",
            "|        Kelly-Warren|\n",
            "|   Reynolds-Sheppard|\n",
            "|          Singh-Cole|\n",
            "|           Lopez PLC|\n",
            "|       Reed-Martinez|\n",
            "|Briggs, Lamb and ...|\n",
            "|    Figueroa-Maynard|\n",
            "|     Abbott-Thompson|\n",
            "|Smith, Kim and Ma...|\n",
            "|Snyder, Lee and M...|\n",
            "|      Sanders-Pierce|\n",
            "|Andrews, Adams an...|\n",
            "|Morgan, Phillips ...|\n",
            "|      Villanueva LLC|\n",
            "|Berry, Orr and Ca...|\n",
            "|       Parks-Bradley|\n",
            "|           Olsen LLC|\n",
            "|Clark, Campbell a...|\n",
            "|          Dalton LLC|\n",
            "|Thompson, Hansen ...|\n",
            "|Yates, Martinez a...|\n",
            "|       Reeves-Curtis|\n",
            "|           Gates Ltd|\n",
            "|     Dunlap and Sons|\n",
            "|Taylor, Allen and...|\n",
            "+--------------------+\n",
            "only showing top 30 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7L0DaJteuc0",
        "outputId": "3265a7f3-f091-434f-d572-5b0e4dce11ca"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Names',\n",
              " 'Age',\n",
              " 'Total_Purchase',\n",
              " 'Account_Manager',\n",
              " 'Years',\n",
              " 'Num_Sites',\n",
              " 'Onboard_date',\n",
              " 'Location',\n",
              " 'Company',\n",
              " 'Churn']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Columns selection \n",
        "my_cols = training.select([\n",
        " 'Age',\n",
        " 'Total_Purchase',\n",
        " 'Account_Manager',\n",
        " 'Years',\n",
        " 'Num_Sites',\n",
        " 'Churn'])"
      ],
      "metadata": {
        "id": "30fQpDq9e0s7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#feature engineering/data cleanup\n",
        "my_final_data = my_cols.na.drop()"
      ],
      "metadata": {
        "id": "1Bo5iMg6f04u"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import (VectorAssembler,VectorIndexer,\n",
        "                                OneHotEncoder,StringIndexer)\n",
        "\n",
        "assembler = VectorAssembler(inputCols=\n",
        "['Age',\n",
        " 'Total_Purchase',\n",
        " 'Account_Manager',\n",
        " 'Years',\n",
        " 'Num_Sites',\n",
        " 'Churn'],outputCol='features')"
      ],
      "metadata": {
        "id": "i6ecXJ1hf2pb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline"
      ],
      "metadata": {
        "id": "tdt6VpL0guN8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_reg_historical = LogisticRegression(featuresCol='features',labelCol='Churn')"
      ],
      "metadata": {
        "id": "YrJg9arpg6ly"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(stages=[assembler,log_reg_historical])"
      ],
      "metadata": {
        "id": "ZyZUZ2gqhyzF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_historical_data, test_historical_data = my_final_data.randomSplit([0.7,.3])\n",
        "fit_model = pipeline.fit(train_historical_data)\n",
        "results = fit_model.transform(test_historical_data)"
      ],
      "metadata": {
        "id": "s4zR2uI_iROe"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "my_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction',\n",
        "                                       labelCol='Churn')\n",
        "\n",
        "results.select('Churn','prediction').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIUHciaYiYM7",
        "outputId": "2863becc-3d87-4ed0-9a00-69f01fdcf28f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+\n",
            "|Churn|prediction|\n",
            "+-----+----------+\n",
            "|    1|       1.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    1|       1.0|\n",
            "|    1|       1.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "+-----+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AUC = my_eval.evaluate(results)\n",
        "AUC"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpcDV0fIin4D",
        "outputId": "ab0cc8c8-1402-4753-b413-676f9ed6e2f6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [conda root]",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}